import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import json
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("SMOTE DATA BALANCING FOR SPANISH RESTAURANT REVIEWS")
print("="*70)

# 1. Load the original sample and embeddings
print("\n1. LOADING DATA")
print("-" * 40)

# Load the 10k stratified sample
sample_df = pd.read_pickle("../../data/Madrid_stratified_sample_10000.pkl")
print(f"Original sample loaded: {len(sample_df)} reviews")
print("Original distribution:")
original_counts = sample_df['sentiment'].value_counts()
print(original_counts)

# Load the BERT embeddings for neutral and negative reviews
embeddings = np.load("../../data/Madrid_neutral_negative_bert_embeddings.npy")
neutral_negative_df = pd.read_pickle("../../data/Madrid_neutral_negative_sample.pkl")
print(f"\nBERT embeddings loaded: {embeddings.shape}")
print(f"Neutral + Negative samples: {len(neutral_negative_df)}")

# 2. Prepare data for SMOTE
print("\n2. PREPARING DATA FOR SMOTE")
print("-" * 40)

# Get only negative and neutral samples with their embeddings
X = embeddings  # BERT embeddings (2683, 768)
y = neutral_negative_df['sentiment'].values  # Labels

print(f"Features (X) shape: {X.shape}")
print(f"Labels (y) shape: {y.shape}")
print("Class distribution before SMOTE:")
class_counts = Counter(y)
for sentiment, count in class_counts.items():
    print(f"  {sentiment}: {count}")

# Calculate target sizes to match positive class (7317)
positive_count = original_counts['positive']
print(f"\nTarget size for balancing: {positive_count} samples per class")

# 3. Apply SMOTE
print("\n3. APPLYING SMOTE")
print("-" * 40)

# Create SMOTE sampler
# We want to oversample to match the positive class size
sampling_strategy = {
    'negative': positive_count,
    'neutral': positive_count
}

smote = SMOTE(
    sampling_strategy=sampling_strategy,
    random_state=42,
    k_neighbors=5
)

print("Applying SMOTE with strategy:")
for sentiment, target_count in sampling_strategy.items():
    original_count = class_counts[sentiment]
    synthetic_count = target_count - original_count
    print(f"  {sentiment}: {original_count} → {target_count} (+{synthetic_count} synthetic)")

# Apply SMOTE
X_resampled, y_resampled = smote.fit_resample(X, y)

print(f"\nSMOTE completed!")
print(f"Original shape: {X.shape}")
print(f"Resampled shape: {X_resampled.shape}")

# Check new distribution
resampled_counts = Counter(y_resampled)
print("\nClass distribution after SMOTE:")
for sentiment, count in resampled_counts.items():
    print(f"  {sentiment}: {count}")

# 4. Create balanced dataset
print("\n4. CREATING BALANCED DATASET")
print("-" * 40)

# Combine original positive reviews with SMOTE-generated neutral/negative reviews
positive_df = sample_df[sample_df['sentiment'] == 'positive'].copy()

# Create DataFrame for SMOTE samples
# First, separate original and synthetic samples
original_indices = list(range(len(y)))
synthetic_indices = list(range(len(y), len(y_resampled)))

# Create synthetic samples DataFrame
synthetic_samples = []
synthetic_embeddings = []

for i, (idx, sentiment) in enumerate(zip(range(len(y_resampled)), y_resampled)):
    if idx >= len(y):  # This is a synthetic sample
        synthetic_samples.append({
            'reviewId': f'SMOTE_{sentiment}_{idx-len(y)+1}',
            'text': f'[SYNTHETIC_{sentiment.upper()}_REVIEW_{idx-len(y)+1}]',
            'text_clean': f'synthetic {sentiment} review generated by smote',
            'rating': 10 if sentiment == 'negative' else (30 if sentiment == 'neutral' else 50),
            'sentiment': sentiment,
            'text_length': 50,  # Placeholder
            'word_count': 8,    # Placeholder
            'date': '2023-SMOTE',
            'synthetic': True
        })
        synthetic_embeddings.append(X_resampled[i])

# Convert to DataFrame
synthetic_df = pd.DataFrame(synthetic_samples)
synthetic_embeddings = np.array(synthetic_embeddings)

# Combine original negative/neutral with synthetic ones
original_negative_neutral = neutral_negative_df.copy()
original_negative_neutral['synthetic'] = False

# Combine all data
balanced_df = pd.concat([
    positive_df.assign(synthetic=False),
    original_negative_neutral,
    synthetic_df
], ignore_index=True)

print(f"Balanced dataset created:")
print(f"  Total samples: {len(balanced_df)}")
print(f"  Original samples: {len(positive_df) + len(original_negative_neutral)}")
print(f"  Synthetic samples: {len(synthetic_df)}")
print("\nFinal class distribution:")
final_counts = balanced_df['sentiment'].value_counts()
print(final_counts)

# 5. Save results
print("\n5. SAVING RESULTS")
print("-" * 40)

# Save balanced dataset
balanced_file = "../../data/Madrid_balanced_smote_dataset.pkl"
balanced_df.to_pickle(balanced_file)
print(f"✓ Balanced dataset saved: {balanced_file}")

balanced_csv = "../../data/Madrid_balanced_smote_dataset.csv"
balanced_df.to_csv(balanced_csv, index=False)
print(f"✓ Balanced dataset CSV: {balanced_csv}")

# Save all embeddings (original + synthetic)
# Combine positive embeddings (need to generate), original neutral/negative, and synthetic
print("\nGenerating embeddings for complete balanced dataset...")

# For now, save the SMOTE-resampled embeddings for neutral/negative classes
smote_embeddings_file = "../../data/Madrid_smote_embeddings.npy"
np.save(smote_embeddings_file, X_resampled)
print(f"✓ SMOTE embeddings saved: {smote_embeddings_file}")

# Save synthetic samples separately for analysis
synthetic_file = "../../data/Madrid_synthetic_smote_samples.pkl"
synthetic_df.to_pickle(synthetic_file)
print(f"✓ Synthetic samples saved: {synthetic_file}")

# Save metadata
metadata = {
    'method': 'SMOTE',
    'original_distribution': {k: int(v) for k, v in original_counts.items()},
    'balanced_distribution': {k: int(v) for k, v in final_counts.items()},
    'synthetic_samples_generated': int(len(synthetic_df)),
    'smote_parameters': {
        'k_neighbors': 5,
        'random_state': 42,
        'sampling_strategy': {k: int(v) for k, v in sampling_strategy.items()}
    },
    'embedding_dimension': int(X.shape[1]),
    'total_balanced_samples': int(len(balanced_df)),
    'files_created': [balanced_file, balanced_csv, smote_embeddings_file, synthetic_file]
}

metadata_file = "../../data/Madrid_smote_metadata.json"
with open(metadata_file, 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"✓ Metadata saved: {metadata_file}")

# 6. Create visualizations
print("\n6. CREATING VISUALIZATIONS")
print("-" * 40)

# Create comprehensive visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('SMOTE Data Balancing Results', fontsize=16, fontweight='bold')

# 1. Original distribution
axes[0, 0].bar(original_counts.index, original_counts.values, 
               color=['#DC143C', '#FFD700', '#2E8B57'], edgecolor='black')
axes[0, 0].set_title('Original Distribution')
axes[0, 0].set_ylabel('Count')
for i, (sentiment, count) in enumerate(original_counts.items()):
    axes[0, 0].text(i, count + 50, f'{count}', ha='center', va='bottom', fontweight='bold')

# 2. Balanced distribution
axes[0, 1].bar(final_counts.index, final_counts.values,
               color=['#DC143C', '#FFD700', '#2E8B57'], edgecolor='black')
axes[0, 1].set_title('Balanced Distribution (After SMOTE)')
axes[0, 1].set_ylabel('Count')
for i, (sentiment, count) in enumerate(final_counts.items()):
    axes[0, 1].text(i, count + 100, f'{count}', ha='center', va='bottom', fontweight='bold')

# 3. Synthetic vs Original breakdown
synthetic_counts = balanced_df.groupby(['sentiment', 'synthetic']).size().unstack(fill_value=0)
synthetic_counts.plot(kind='bar', stacked=True, ax=axes[0, 2], 
                     color=['lightblue', 'orange'], edgecolor='black')
axes[0, 2].set_title('Original vs Synthetic Samples')
axes[0, 2].set_ylabel('Count')
axes[0, 2].legend(['Original', 'Synthetic'])
axes[0, 2].tick_params(axis='x', rotation=0)

# 4. Class balance improvement
improvement_data = pd.DataFrame({
    'Original': original_counts.reindex(['negative', 'neutral', 'positive'], fill_value=0),
    'Balanced': final_counts.reindex(['negative', 'neutral', 'positive'], fill_value=0)
})
improvement_data.plot(kind='bar', ax=axes[1, 0], color=['lightcoral', 'lightgreen'], edgecolor='black')
axes[1, 0].set_title('Class Balance Improvement')
axes[1, 0].set_ylabel('Count')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].legend()

# 5. PCA visualization of embeddings (sample for performance)
print("  Computing PCA visualization...")
n_samples_viz = min(1000, len(X_resampled))
sample_indices = np.random.choice(len(X_resampled), n_samples_viz, replace=False)
X_sample = X_resampled[sample_indices]
y_sample = np.array(y_resampled)[sample_indices]

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_sample)

colors = {'negative': '#DC143C', 'neutral': '#FFD700', 'positive': '#2E8B57'}
for sentiment in np.unique(y_sample):
    mask = y_sample == sentiment
    axes[1, 1].scatter(X_pca[mask, 0], X_pca[mask, 1], 
                      c=colors[sentiment], label=sentiment, alpha=0.6, s=20)

axes[1, 1].set_title(f'PCA of SMOTE Embeddings (n={n_samples_viz})')
axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
axes[1, 1].legend()

# 6. Imbalance metrics comparison
imbalance_before = max(original_counts) / min(original_counts)
imbalance_after = max(final_counts) / min(final_counts)

metrics_df = pd.DataFrame({
    'Before SMOTE': [imbalance_before, original_counts.std()],
    'After SMOTE': [imbalance_after, final_counts.std()]
}, index=['Imbalance Ratio', 'Standard Deviation'])

metrics_df.plot(kind='bar', ax=axes[1, 2], color=['lightcoral', 'lightgreen'], edgecolor='black')
axes[1, 2].set_title('Imbalance Metrics Comparison')
axes[1, 2].set_ylabel('Value')
axes[1, 2].tick_params(axis='x', rotation=45)
axes[1, 2].legend()

plt.tight_layout()
plt.savefig('../../data/Madrid_SMOTE_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("✓ Visualizations created and saved")

# 7. Summary statistics
print("\n7. FINAL SUMMARY")
print("=" * 40)
print(f"SMOTE Balancing Completed Successfully!")
print(f"\nOriginal dataset: {sum(original_counts)} samples")
print(f"Balanced dataset: {len(balanced_df)} samples")
print(f"Synthetic samples generated: {len(synthetic_df)}")
print(f"\nImbalance improvement:")
print(f"  Before: {imbalance_before:.2f}:1 ratio")
print(f"  After: {imbalance_after:.2f}:1 ratio")
print(f"  Improvement: {((imbalance_before - imbalance_after) / imbalance_before * 100):.1f}%")

print(f"\nFiles created:")
for file in metadata['files_created']:
    print(f"  ✓ {file}")
print(f"  ✓ Madrid_SMOTE_analysis.png")
print(f"  ✓ {metadata_file}")

print("\n" + "="*70)
print("SMOTE BALANCING PROCESS COMPLETED!")
print("="*70)